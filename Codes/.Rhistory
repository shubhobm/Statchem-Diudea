mod508.gbm = train(as.factor(y508)~., data=df508[train,], method="gbm", trControl=myControl, tuneGrid=myGrid,verbose=F)
pred.gbm = predict(mod508.gbm, df508[-train,], type="prob")[,1]
cat("Split",split,"done!\n")
c(as.numeric(auc(roc(y508[-train],pred.PCR))),
as.numeric(auc(roc(y508[-train],pred.PLS))),
as.numeric(auc(roc(y508[-train],as.numeric(pred.lasso)))),
as.numeric(auc(roc(y508[-train],as.numeric(pred.SCAD)))),
as.numeric(auc(roc(y508[-train],pred.rf))),
as.numeric(auc(roc(y508[-train],pred.gbm))))
}
loopfun(1)
err.mat = lapply(1:nsplit, loopfun)
nsplit
nsplit=2
err.mat = lapply(1:nsplit, loopfun)
?plsRglm
loopfun = function(split){
set.seed(split*12012017)
train = sample(1:n, ceiling(.8*n), replace=F)
ntrain = length(train)
## Principal Component Regression
Xd = X508[train,]
depth = depth.projection(X508[train,], X508[train,])
depth = max(depth) - depth
for(i in 1:ntrain)
{
z = sqrt(sum((Xd[i,  ])^2))
if(z > ep)
{
Xd[i,  ] = depth[i] * (Xd[i,  ]  )/z
}
}
svd508 = svd(Xd)
npc = min(which(cumsum(svd508$d/sum(svd508$d)) >= .95))
Xt.train = X508[train,] %*% svd508$v[,1:npc]
mod508.PCR = glm(X1~., family="binomial", data.frame(cbind(y508[train], Xt.train)))
pred.PCR = predict(mod508.PCR, newdata=data.frame(cbind(y508[-train], X508[-train,] %*% svd508$v[,1:npc])),
type="response")
cat("PCR done\n")
## PLS regression
mod508.PLS = plsRglm(y508~., data=df508[train,], nt=10, family="binomial", modele="pls-glm-family", verbose=F)
pred.PLS = predict(mod508.PLS, newdata=df508[-train,-1])
cat("PLS done\n")
## LS-LASSO
mod508.lasso = cv.glmnet(X508[train,], y508[train], nfolds=10, family="binomial")
beta.lasso = as.numeric(coef(mod508.lasso), s="lambda.min")
pred.lasso = exp(cbind(1, X508[-train,]) %*% beta.lasso)
pred.lasso = pred.lasso/(1+pred.lasso)
cat("Lasso done\n")
## LS-SCAD
mod508.SCAD = cv.ncvreg(X508[train,], y508[train], family="binomial", penalty="SCAD")
beta.SCAD = mod508.SCAD$fit$beta[,which.min(mod508.SCAD$cve)]
pred.SCAD = cbind(1, X508[-train,]) %*% beta.SCAD
pred.SCAD = pred.lasso/(1+pred.SCAD)
cat("SCAD done\n")
## random forest
mod508.rf = randomForest(as.factor(y508)~., data=df508[train,])
pred.rf = predict(mod508.rf, df508[-train,], type="prob")[,1]
cat("rf done\n")
## Gradient boosting, will train using caret
myControl = trainControl(method="cv", number=5)
myGrid = expand.grid(n.trees=c(100,500,1e3), interaction.depth=c(1,2), shrinkage=c(.1,.01,.001), n.minobsinnode=c(1,2,5))
mod508.gbm = train(as.factor(y508)~., data=df508[train,], method="gbm", trControl=myControl, tuneGrid=myGrid,verbose=F)
pred.gbm = predict(mod508.gbm, df508[-train,], type="prob")[,1]
cat("gbm done\n")
cat("Split",split,"done!===========\n")
c(as.numeric(auc(roc(y508[-train],pred.PCR))),
as.numeric(auc(roc(y508[-train],pred.PLS))),
as.numeric(auc(roc(y508[-train],as.numeric(pred.lasso)))),
as.numeric(auc(roc(y508[-train],as.numeric(pred.SCAD)))),
as.numeric(auc(roc(y508[-train],pred.rf))),
as.numeric(auc(roc(y508[-train],pred.gbm))))
}
nsplit = 2
err.mat = lapply(1:nsplit, loopfun)
loopfun = function(split){
set.seed(split*12012017)
train = sample(1:n, ceiling(.8*n), replace=F)
ntrain = length(train)
## Principal Component Regression
Xd = X508[train,]
depth = depth.projection(X508[train,], X508[train,])
depth = max(depth) - depth
for(i in 1:ntrain)
{
z = sqrt(sum((Xd[i,  ])^2))
if(z > ep)
{
Xd[i,  ] = depth[i] * (Xd[i,  ]  )/z
}
}
svd508 = svd(Xd)
npc = min(which(cumsum(svd508$d/sum(svd508$d)) >= .95))
Xt.train = X508[train,] %*% svd508$v[,1:npc]
mod508.PCR = glm(X1~., family="binomial", data.frame(cbind(y508[train], Xt.train)))
pred.PCR = predict(mod508.PCR, newdata=data.frame(cbind(y508[-train], X508[-train,] %*% svd508$v[,1:npc])),
type="response")
cat("PCR done\n")
## PLS regression
mod508.PLS = plsRglm(y508~., data=df508[train,], nt=10, family="binomial", modele="pls-glm-family", verbose=F)
pred.PLS = predict(mod508.PLS, newdata=df508[-train,-1])
cat("PLS done\n")
## LS-LASSO
mod508.lasso = cv.glmnet(X508[train,], y508[train], nfolds=10, family="binomial")
beta.lasso = as.numeric(coef(mod508.lasso), s="lambda.min")
pred.lasso = exp(cbind(1, X508[-train,]) %*% beta.lasso)
pred.lasso = pred.lasso/(1+pred.lasso)
cat("Lasso done\n")
## LS-SCAD
mod508.SCAD = cv.ncvreg(X508[train,], y508[train], family="binomial", penalty="SCAD")
beta.SCAD = mod508.SCAD$fit$beta[,which.min(mod508.SCAD$cve)]
pred.SCAD = cbind(1, X508[-train,]) %*% beta.SCAD
pred.SCAD = pred.lasso/(1+pred.SCAD)
cat("SCAD done\n")
## random forest
mod508.rf = randomForest(as.factor(y508)~., data=df508[train,])
pred.rf = predict(mod508.rf, df508[-train,], type="prob")[,1]
cat("rf done\n")
## Gradient boosting, will train using caret
myControl = trainControl(method="cv", number=5)
myGrid = expand.grid(n.trees=c(100,500), interaction.depth=c(1,2), shrinkage=c(.1,.01,.001), n.minobsinnode=c(1,2,5))
mod508.gbm = train(as.factor(y508)~., data=df508[train,], method="gbm", trControl=myControl, tuneGrid=myGrid,verbose=F)
pred.gbm = predict(mod508.gbm, df508[-train,], type="prob")[,1]
cat("gbm done\n")
cat("Split",split,"done!===========\n")
c(as.numeric(auc(roc(y508[-train],pred.PCR))),
as.numeric(auc(roc(y508[-train],pred.PLS))),
as.numeric(auc(roc(y508[-train],as.numeric(pred.lasso)))),
as.numeric(auc(roc(y508[-train],as.numeric(pred.SCAD)))),
as.numeric(auc(roc(y508[-train],pred.rf))),
as.numeric(auc(roc(y508[-train],pred.gbm))))
}
nsplit = 2
err.mat = lapply(1:nsplit, loopfun)
err.mat
err.mat = matrix(unlist(err.mat), nrow=6, byrow=T)
err.mat
loopfun = function(split){
set.seed(split*12012017)
train = sample(1:n, ceiling(.8*n), replace=F)
ntrain = length(train)
## Principal Component Regression
Xd = X508[train,]
depth = depth.projection(X508[train,], X508[train,])
depth = max(depth) - depth
for(i in 1:ntrain)
{
z = sqrt(sum((Xd[i,  ])^2))
if(z > ep)
{
Xd[i,  ] = depth[i] * (Xd[i,  ]  )/z
}
}
svd508 = svd(Xd)
npc = min(which(cumsum(svd508$d/sum(svd508$d)) >= .95))
Xt.train = X508[train,] %*% svd508$v[,1:npc]
mod508.PCR = glm(X1~., family="binomial", data.frame(cbind(y508[train], Xt.train)))
pred.PCR = predict(mod508.PCR, newdata=data.frame(cbind(y508[-train], X508[-train,] %*% svd508$v[,1:npc])),
type="response")
cat("PCR done\n")
## PLS regression
mod508.PLS = plsRglm(y508~., data=df508[train,], nt=10, family="binomial", modele="pls-glm-family", verbose=F)
pred.PLS = predict(mod508.PLS, newdata=df508[-train,-1])
cat("PLS done\n")
## LS-LASSO
mod508.lasso = cv.glmnet(X508[train,], y508[train], nfolds=10, family="binomial")
beta.lasso = as.numeric(coef(mod508.lasso), s="lambda.min")
pred.lasso = exp(cbind(1, X508[-train,]) %*% beta.lasso)
pred.lasso = pred.lasso/(1+pred.lasso)
cat("Lasso done\n")
## LS-SCAD
mod508.SCAD = cv.ncvreg(X508[train,], y508[train], family="binomial", penalty="SCAD")
beta.SCAD = mod508.SCAD$fit$beta[,which.min(mod508.SCAD$cve)]
pred.SCAD = cbind(1, X508[-train,]) %*% beta.SCAD
pred.SCAD = pred.lasso/(1+pred.SCAD)
cat("SCAD done\n")
## random forest
mod508.rf = randomForest(as.factor(y508)~., data=df508[train,])
pred.rf = predict(mod508.rf, df508[-train,], type="prob")[,1]
cat("rf done\n")
## Gradient boosting, will train using caret
myControl = trainControl(method="cv", number=5)
myGrid = expand.grid(n.trees=c(100,500,1e3), interaction.depth=c(1,2), shrinkage=c(.1,.01,.001), n.minobsinnode=c(1,2,5))
mod508.gbm = train(as.factor(y508)~., data=df508[train,], method="gbm", trControl=myControl, tuneGrid=myGrid,verbose=F)
pred.gbm = predict(mod508.gbm, df508[-train,], type="prob")[,1]
cat("gbm done\n")
cat("Split",split,"done!===========\n")
c(as.numeric(auc(roc(y508[-train],pred.PCR))),
as.numeric(auc(roc(y508[-train],pred.PLS))),
as.numeric(auc(roc(y508[-train],as.numeric(pred.lasso)))),
as.numeric(auc(roc(y508[-train],as.numeric(pred.SCAD)))),
as.numeric(auc(roc(y508[-train],pred.rf))),
as.numeric(auc(roc(y508[-train],pred.gbm))))
}
nsplit = 2
err.mat = lapply(1:nsplit, loopfun)
err.mat1 = matrix(unlist(err.mat), ncol=6, byrow=T)
err.mat1
loopfun(1)
nsplit = 1e2
err.mat = mclapply(1:nsplit, loopfun, mc.cores=min(detectCores(),nsplit))
err.mat = lapply(1:nsplit, loopfun)
err.mat1 = matrix(unlist(err.mat), ncol=6, byrow=T)
err.mat1
colMeans(err.mat1)
save(err.mat1, file="err508.Rda")
rm(list=ls())
setwd('D:/Study/My projects/Statchem-Diudea/Codes')
source('RobustQSAR_functions.R')
library(glmnet)
library(ddalpha)
library(pls)
library(ncvreg)
library(randomForest)
library(caret)
# combined descriptors
load('../Data/lta98.rda')
y95 = lta98$Y[-1]
X95 = as.matrix(with(lta98, cbind(ltaTS,ltaTC,lta3D, ltaQC))[-1,])
delta = 1e-3
spa = spatial.median(X95, delta)
mu = spa$mu
ep = spa$ep
sigma.vec = apply(X95, 2, mad)
X95 = as.matrix(scale(X95, mu, sigma.vec))
X95 = X95[,-which(is.na(apply(X95,2,var)))]
df95 = data.frame(cbind(y95, X95))
n = nrow(X95)
p = ncol(X95)
dim(X95)
dim(Y95)
nsplit = 110
err.mat = matrix(0, nrow=nsplit, ncol=6)
for(split in 1:nsplit){
set.seed(split*12012017)
train = sample(1:n, ceiling(.8*n), replace=F)
ntrain = length(train)
## Principal Component Regression
Xd = X95[train,]
depth = depth.projection(X95[train,], X95[train,])
depth = max(depth) - depth
for(i in 1:ntrain)
{
z = sqrt(sum((Xd[i,  ])^2))
if(z > ep)
{
Xd[i,  ] = depth[i] * (Xd[i,  ]  )/z
}
}
svd95 = svd(Xd)
npc = min(which(cumsum(svd95$d/sum(svd95$d)) >= .95))
Xt.train = X95[train,] %*% svd95$v[,1:npc]
mod95.PCR = lm(X1~., data.frame(cbind(y95[train], Xt.train)))
pred.PCR = predict(mod95.PCR, newdata=data.frame(cbind(y95[-train], X95[-train,] %*% svd95$v[,1:npc])))
## PLS regression
mod95.PLS = plsr(y95~., data=df95[train,], validation="CV")
pred.PLS = predict(mod95.PLS, df95[-train,])
err.PLS = min(as.numeric(lapply(1:dim(pred.PLS)[3], function(x) sum((y95[-train] - pred.PLS[,,x])^2))))
## LS-LASSO
mod95.lasso = cv.glmnet(X95[train,], y95[train], nfolds=10)
beta.lasso = as.numeric(coef(mod95.lasso), s="lambda.min")
pred.lasso = cbind(1, X95[-train,]) %*% beta.lasso
## LS-SCAD
mod95.SCAD = cv.ncvreg(X95[train,], y95[train], family="gaussian", penalty="SCAD")
beta.SCAD = mod95.SCAD$fit$beta[,which.min(mod95.SCAD$cve)]
pred.SCAD = cbind(1, X95[-train,]) %*% beta.SCAD
## random forest
mod95.rf = randomForest(y95~., data=df95[train,])
pred.rf = predict(mod95.rf, df95[-train,])
## Gradient boosting, will train using caret
myControl = trainControl(method="cv", number=5)
myGrid = expand.grid(n.trees=c(100,500,1e3), interaction.depth=c(1,2), shrinkage=c(.1,.01,.001), n.minobsinnode=c(1,2,5))
mod95.gbm = train(y95~., data=df95[train,], method="gbm", trControl=myControl, tuneGrid=myGrid,verbose=F)
pred.gbm = predict(mod95.gbm, df95[-train,])
err.mat[split,] = c(sum((y95[-train] - pred.PCR)^2),
err.PLS,
sum((y95[-train] - pred.lasso)^2),
sum((y95[-train] - pred.SCAD)^2),
sum((y95[-train] - pred.rf)^2),
sum((y95[-train] - pred.gbm)^2))
cat("Split",split,"done!\n")
}
err.mat
save(err.mat, file="err95basak.Rda")
setwd('D:/Study/My projects/Statchem-Diudea/Codes')
library(glmnet)
library(ddalpha)
library(plsRglm)
library(ncvreg)
library(randomForest)
library(caret)
library(pROC)
library(parallel)
basak508 = read.csv("../Data/Basak-descriptors-508.csv")
basak508[1,]
dim(basak508)
X508 = as.numeric(basak508[-1,-c(1,309)])
X508 = as.matrix(basak508[-1,-c(1,309)])
dim(X508)
y508 = as.numeric(basak508[-1,309])
length(y508)
# apply robust scaling
set.seed(11122017)
delta = 1e-3
spa = spatial.median(X508, delta)
mu = spa$mu
ep = spa$ep
sigma.vec = apply(X508, 2, mad)
X508 = as.matrix(scale(X508, mu, sigma.vec))
X508 = X508[,-which(is.na(apply(X508,2,var)))]
df508 = data.frame(cbind(y508, X508))
n = nrow(X508)
p = ncol(X508)
source('RobustQSAR_functions.R')
set.seed(11122017)
delta = 1e-3
spa = spatial.median(X508, delta)
mu = spa$mu
ep = spa$ep
sigma.vec = apply(X508, 2, mad)
X508 = as.matrix(scale(X508, mu, sigma.vec))
X508 = X508[,-which(is.na(apply(X508,2,var)))]
df508 = data.frame(cbind(y508, X508))
n = nrow(X508)
p = ncol(X508)
loopfun = function(split){
set.seed(split*12012017)
train = sample(1:n, ceiling(.8*n), replace=F)
ntrain = length(train)
## Principal Component Regression
Xd = X508[train,]
depth = depth.projection(X508[train,], X508[train,])
depth = max(depth) - depth
for(i in 1:ntrain)
{
z = sqrt(sum((Xd[i,  ])^2))
if(z > ep)
{
Xd[i,  ] = depth[i] * (Xd[i,  ]  )/z
}
}
svd508 = svd(Xd)
npc = min(which(cumsum(svd508$d/sum(svd508$d)) >= .95))
Xt.train = X508[train,] %*% svd508$v[,1:npc]
mod508.PCR = glm(X1~., family="binomial", data.frame(cbind(y508[train], Xt.train)))
pred.PCR = predict(mod508.PCR, newdata=data.frame(cbind(y508[-train], X508[-train,] %*% svd508$v[,1:npc])),
type="response")
cat("PCR done\n")
## PLS regression
mod508.PLS = plsRglm(y508~., data=df508[train,], nt=10, family="binomial", modele="pls-glm-family", verbose=F)
pred.PLS = predict(mod508.PLS, newdata=df508[-train,-1])
cat("PLS done\n")
## LS-LASSO
mod508.lasso = cv.glmnet(X508[train,], y508[train], nfolds=10, family="binomial")
beta.lasso = as.numeric(coef(mod508.lasso), s="lambda.min")
pred.lasso = exp(cbind(1, X508[-train,]) %*% beta.lasso)
pred.lasso = pred.lasso/(1+pred.lasso)
cat("Lasso done\n")
## LS-SCAD
mod508.SCAD = cv.ncvreg(X508[train,], y508[train], family="binomial", penalty="SCAD")
beta.SCAD = mod508.SCAD$fit$beta[,which.min(mod508.SCAD$cve)]
pred.SCAD = cbind(1, X508[-train,]) %*% beta.SCAD
pred.SCAD = pred.lasso/(1+pred.SCAD)
cat("SCAD done\n")
## random forest
mod508.rf = randomForest(as.factor(y508)~., data=df508[train,])
pred.rf = predict(mod508.rf, df508[-train,], type="prob")[,1]
cat("rf done\n")
## Gradient boosting, will train using caret
myControl = trainControl(method="cv", number=5)
myGrid = expand.grid(n.trees=c(100,500,1e3), interaction.depth=c(1,2), shrinkage=c(.1,.01,.001), n.minobsinnode=c(1,2,5))
mod508.gbm = train(as.factor(y508)~., data=df508[train,], method="gbm", trControl=myControl, tuneGrid=myGrid,verbose=F)
pred.gbm = predict(mod508.gbm, df508[-train,], type="prob")[,1]
cat("gbm done\n")
cat("Split",split,"done!===========\n")
c(as.numeric(auc(roc(y508[-train],pred.PCR))),
as.numeric(auc(roc(y508[-train],pred.PLS))),
as.numeric(auc(roc(y508[-train],as.numeric(pred.lasso)))),
as.numeric(auc(roc(y508[-train],as.numeric(pred.SCAD)))),
as.numeric(auc(roc(y508[-train],pred.rf))),
as.numeric(auc(roc(y508[-train],pred.gbm))))
}
nsplit = 1e2
err.mat = lapply(1:nsplit, loopfun)
class(X508)
set.seed(split*12012017)
train = sample(1:n, ceiling(.8*n), replace=F)
ntrain = length(train)
## Principal Component Regression
Xd = X508[train,]
depth = depth.projection(X508[train,], X508[train,])
dim(X508[train,])
m
n
train = sample(1:n, ceiling(.8*n), replace=F)
ntrain
Xd = X508[train,]
depth = depth.projection(X508[train,], X508[train,])
dim(X508[train, ])
dim(X508)
y508 = as.numeric(basak508[-1,309])
X508 = as.matrix(basak508[-1,-c(1,309)])
# apply robust scaling
set.seed(11122017)
delta = 1e-3
spa = spatial.median(X508, delta)
mu = spa$mu
ep = spa$ep
sigma.vec = apply(X508, 2, mad)
X508 = as.matrix(scale(X508, mu, sigma.vec))
X508 = X508[,-which(is.na(apply(X508,2,var)))]
df508 = data.frame(cbind(y508, X508))
n = nrow(X508)
p = ncol(X508)
n
p
dim(X508)
set.seed(split*12012017)
train = sample(1:n, ceiling(.8*n), replace=F)
ntrain = length(train)
Xd = X508[train,]
depth = depth.projection(X508[train,], X508[train,])
loopfun = function(split){
set.seed(split*12012017)
train = sample(1:n, ceiling(.8*n), replace=F)
ntrain = length(train)
## Principal Component Regression
Xd = X508[train,]
depth = depth.projection(X508[train,], X508[train,])
depth = max(depth) - depth
for(i in 1:ntrain)
{
z = sqrt(sum((Xd[i,  ])^2))
if(z > ep)
{
Xd[i,  ] = depth[i] * (Xd[i,  ]  )/z
}
}
svd508 = svd(Xd)
npc = min(which(cumsum(svd508$d/sum(svd508$d)) >= .95))
Xt.train = X508[train,] %*% svd508$v[,1:npc]
mod508.PCR = glm(X1~., family="binomial", data.frame(cbind(y508[train], Xt.train)))
pred.PCR = predict(mod508.PCR, newdata=data.frame(cbind(y508[-train], X508[-train,] %*% svd508$v[,1:npc])),
type="response")
cat("PCR done\n")
## PLS regression
mod508.PLS = plsRglm(y508~., data=df508[train,], nt=10, family="binomial", modele="pls-glm-family", verbose=F)
pred.PLS = predict(mod508.PLS, newdata=df508[-train,-1])
cat("PLS done\n")
## LS-LASSO
mod508.lasso = cv.glmnet(X508[train,], y508[train], nfolds=10, family="binomial")
beta.lasso = as.numeric(coef(mod508.lasso), s="lambda.min")
pred.lasso = exp(cbind(1, X508[-train,]) %*% beta.lasso)
pred.lasso = pred.lasso/(1+pred.lasso)
cat("Lasso done\n")
## LS-SCAD
mod508.SCAD = cv.ncvreg(X508[train,], y508[train], family="binomial", penalty="SCAD")
beta.SCAD = mod508.SCAD$fit$beta[,which.min(mod508.SCAD$cve)]
pred.SCAD = cbind(1, X508[-train,]) %*% beta.SCAD
pred.SCAD = pred.lasso/(1+pred.SCAD)
cat("SCAD done\n")
## random forest
mod508.rf = randomForest(as.factor(y508)~., data=df508[train,])
pred.rf = predict(mod508.rf, df508[-train,], type="prob")[,1]
cat("rf done\n")
## Gradient boosting, will train using caret
myControl = trainControl(method="cv", number=5)
myGrid = expand.grid(n.trees=c(100,500,1e3), interaction.depth=c(1,2), shrinkage=c(.1,.01,.001), n.minobsinnode=c(1,2,5))
mod508.gbm = train(as.factor(y508)~., data=df508[train,], method="gbm", trControl=myControl, tuneGrid=myGrid,verbose=F)
pred.gbm = predict(mod508.gbm, df508[-train,], type="prob")[,1]
cat("gbm done\n")
cat("Split",split,"done!===========\n")
c(as.numeric(auc(roc(y508[-train],pred.PCR))),
as.numeric(auc(roc(y508[-train],pred.PLS))),
as.numeric(auc(roc(y508[-train],as.numeric(pred.lasso)))),
as.numeric(auc(roc(y508[-train],as.numeric(pred.SCAD)))),
as.numeric(auc(roc(y508[-train],pred.rf))),
as.numeric(auc(roc(y508[-train],pred.gbm))))
}
nsplit = 1e2
# err.mat = mclapply(1:nsplit, loopfun, mc.cores=min(detectCores(),nsplit))
err.mat = lapply(1:nsplit, loopfun)
err.mat = lapply(1:nsplit, loopfun)
loopfun = function(split){
set.seed(split*12012017)
train = sample(1:n, ceiling(.8*n), replace=F)
ntrain = length(train)
## PLS regression
mod508.PLS = plsRglm(y508~., data=df508[train,], nt=10, family="binomial", modele="pls-glm-family", verbose=F)
pred.PLS = predict(mod508.PLS, newdata=df508[-train,-1])
cat("Split",split,"done!\n")
as.numeric(auc(roc(y508[-train],pred.PLS)))
}
nsplit = 1e2
err.mat = as.numeric(lapply(1:nsplit, loopfun))
err.mat
setwd('D:/Study/My projects/Statchem-Diudea/Codes')
load('err508basak.Rda')
err.mat
load('err508basak.Rda')
err.mat1
err.mat508 = cbind(err.mat1[,1],err.mat,err.mat1[,-1])
save(err.mat508, file="err508basak-final.Rda")
err.mat508
boxplot(err.mat508)
load("D:/Study/My projects/Statchem-Diudea/Codes/err508.Rda")
boxplot(err.mat1)
load("D:/Study/My projects/Statchem-Diudea/Codes/err95basak.Rda")
boxplot(err.mat1)
boxplot(err.mat)
load("D:/Study/My projects/Statchem-Diudea/Codes/err95.Rda")
boxplot(err.mat)
